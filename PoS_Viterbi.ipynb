{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bb3646-7790-4c58-9ab6-2c484b57a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6ed2f",
   "metadata": {},
   "source": [
    "1. FASE DI MODELLING: Viene fornito un modello formale del problema affrontato (già noto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c00595",
   "metadata": {},
   "source": [
    "2. FASE DI LEARNING: Si cerca di capire come, dato un corpus, sia possibile settare i parametri in grado di generare il modello in grado di apprendere da un corpus.\n",
    "\n",
    "   DEFINIZIONE DELLA MATRICE DI TRANSIZIONE E DELLA MATRICE DI EMISSIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b47ffc-8558-4123-9d47-6322cb21e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(possible_tags, train):\n",
    "    transition_matrix = np.zeros((len(possible_tags), len(possible_tags)), dtype='float32')\n",
    "    \n",
    "    transition_counter_dict = dict()\n",
    "    counter_dict = dict()\n",
    "    count_initial_dict = dict()\n",
    "\n",
    "    #FASE 1\n",
    "    for tag1 in possible_tags:\n",
    "        counter_dict[tag1] = 0\n",
    "        count_initial_dict[tag1] = 0\n",
    "        for tag2 in possible_tags:\n",
    "            transition_counter_dict[(tag1, tag2)] = 0\n",
    "\n",
    "    #FASE 2\n",
    "    sentence_n = 0\n",
    "    for sentence in parse_incr(train):\n",
    "        sentence_n += 1\n",
    "        for i in range(len(sentence)):\n",
    "            word_before = sentence[i-1]\n",
    "            word = sentence[i]\n",
    "            if i == 0:\n",
    "                if word[\"upos\"] in count_initial_dict.keys():\n",
    "                    count_initial_dict[word[\"upos\"]] = count_initial_dict[word[\"upos\"]] + 1\n",
    "            if (word_before[\"upos\"], word[\"upos\"]) in transition_counter_dict.keys() and i != 0:\n",
    "                transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] = transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] + 1\n",
    "            if word[\"upos\"] in counter_dict.keys():\n",
    "                counter_dict[word[\"upos\"]] = counter_dict[word[\"upos\"]] + 1\n",
    "            if i == len(sentence) - 1:\n",
    "                if (word[\"upos\"], 'END') in transition_counter_dict.keys():\n",
    "                    transition_counter_dict[(word[\"upos\"], 'END')] = transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] + 1\n",
    "    \n",
    "    #FASE 3: Calcolo delle probabilità di emissione delle parole iniziali, intermedie e finale per ottenere delle performance migliori\n",
    "    #probabilità di transizione iniziali\n",
    "    for i,t in enumerate(possible_tags):\n",
    "        transition_matrix[0][i] = count_initial_dict[t]/sentence_n\n",
    "    #probabilità di transizione intermedie\n",
    "    for i,t1 in enumerate(possible_tags):\n",
    "        for j,t2 in enumerate(possible_tags):\n",
    "            if i >= 1 and j >= 1 and i < (len(possible_tags) - 1):\n",
    "                transition_matrix[i][j] =  transition_counter_dict[(t1,t2)]/counter_dict[t1]\n",
    "    train.seek(0)\n",
    "    return transition_matrix\n",
    "\n",
    "#una_tantum -> serializzare\n",
    "def compute_emission_probabilities(train):\n",
    "    word_tag_set = []\n",
    "    tags_set = []\n",
    "    words_set = []\n",
    "    for sentence in parse_incr(train):\n",
    "        for token in sentence:\n",
    "            word_tag_set.append((token[\"form\"],token[\"upos\"]))\n",
    "            tags_set.append(token[\"upos\"])\n",
    "            words_set.append(token[\"form\"])\n",
    "            \n",
    "    count_word_tag = dict(Counter(word_tag_set))\n",
    "    count_tags = dict(Counter(tags_set))\n",
    "    count_word = dict(Counter(words_set))\n",
    "    \n",
    "    emission_dict = dict()\n",
    "    for key in count_word_tag:\n",
    "        emission_dict[(key[0],key[1])] = count_word_tag[key]/count_tags[key[1]]\n",
    "    return emission_dict,count_word,count_word_tag\n",
    "\n",
    "#una_tantum -> serializzare\n",
    "def compute_oneshot_words_distributions(possible_tags, dev):\n",
    "    word_tag_set = []\n",
    "    word_set = []\n",
    "    for sentence in parse_incr(dev):\n",
    "        for token in sentence:\n",
    "            word_tag_set.append((token[\"form\"],token[\"upos\"]))\n",
    "            word_set.append(token[\"form\"])\n",
    "    word_tag = dict(word_tag_set)\n",
    "    count_word = dict(Counter(word_set))\n",
    "    one_shot_words_tag = []\n",
    "    for word in [k for k,v in count_word.items() if float(v) == 1]:\n",
    "        one_shot_words_tag.append((word,word_tag[word]))\n",
    "    \n",
    "    tags = []\n",
    "    total_tags = 0\n",
    "    for word,tag in one_shot_words_tag:\n",
    "        tags.append(tag)\n",
    "        total_tags = total_tags + 1\n",
    "    distributions = []\n",
    "    for key,count in dict(Counter(tags)).items():\n",
    "        distributions.append((key,count/total_tags))\n",
    "    for tag in possible_tags:\n",
    "        if tag not in tags:\n",
    "            distributions.append((tag,0))\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe162f",
   "metadata": {},
   "source": [
    "3. FASE DI DECODING: Trovare l'algoritmo che permette di sfruttare al meglio i parametri appresi durante la fase di learning, per poter recuperare la soluzione ottimale dato un certo input.\n",
    "   \n",
    "   ALGORITMO DI VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44df67a-0c8a-4ade-9a3d-7838695e18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_word, smoothing_strategy, oneshot_words_tag_distribution):\n",
    "    \n",
    "    viterbi_matrix = np.zeros((len(possible_tags), len(sentence_tokens))) #matrice di viterbi\n",
    "    backpointer = dict() #dizionario di dizionari\n",
    "    \n",
    "    #FASE 1: inizializzazione della prima colonna\n",
    "    for s,tag in enumerate(possible_tags):\n",
    "        transition_p = transition_matrix.loc['START',tag]\n",
    "        emission_p = get_emission_p(emission_probabilities, sentence_tokens[0], tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags)\n",
    "        \n",
    "        if transition_p == 0 : transition_p = np.finfo(float).tiny\n",
    "        if emission_p == 0 : emission_p = np.finfo(float).tiny\n",
    "        \n",
    "        viterbi_matrix[s,0] = math.log(transition_p) +  math.log(emission_p) \n",
    "        \n",
    "    #FASE 2: Inizializzazione delle colonne intermedie\n",
    "    #Si cicla prima sulle colonne e poi sulle righe \n",
    "    for t in range(1,len(sentence_tokens)):\n",
    "        backpointer_column = dict()\n",
    "        for s, tag in enumerate(possible_tags):\n",
    "            max_ , backpointer_column[s] = get_max_argmax_value(possible_tags, viterbi_matrix, transition_matrix, t, s)\n",
    "            emission_p = get_emission_p(emission_probabilities, sentence_tokens[t], tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags)\n",
    "            if emission_p == 0: emission_p = np.finfo(float).tiny\n",
    "            viterbi_matrix[s,t] = max_ + math.log(emission_p) \n",
    "        backpointer[t] = backpointer_column\n",
    "    \n",
    "    #FASE 2: step finale (argmax)\n",
    "    max_ = -sys.maxsize\n",
    "    best_path_pointer = None\n",
    "    for s,tag in enumerate(possible_tags):\n",
    "        end_transition = transition_matrix.loc[tag,'END']\n",
    "        if end_transition == 0: end_transition = np.finfo(float).tiny\n",
    "        val = viterbi_matrix[s,len(sentence_tokens) - 1] + math.log(end_transition)\n",
    "        if val >= max_: max_ = val ; best_path_pointer = s\n",
    "    \n",
    "    #FASE 3: backtracking\n",
    "    #Recupero tramite backtracking della sequenza di PoS\n",
    "    states = []\n",
    "    states.append(best_path_pointer)\n",
    "    t = len(sentence_tokens) - 1\n",
    "    s = best_path_pointer\n",
    "    while t >= 1:\n",
    "        states.append(backpointer[t].get(s))\n",
    "        s = backpointer[t].get(s)\n",
    "        t = t -1\n",
    "    \n",
    "    #FASE 4: reverse PoS_Tag sequence\n",
    "    pos_tags_sequence = []\n",
    "    for state in list(reversed(states)): pos_tags_sequence.append(possible_tags[state])\n",
    "    return pos_tags_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648389",
   "metadata": {},
   "source": [
    "FUNZIONI DI SUPPORTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0457c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_argmax_value(possible_tags, viterbi_matrix, transition_matrix, t, s):\n",
    "    max_ = -sys.maxsize\n",
    "    argmax = None\n",
    "    for s1, tag in enumerate(possible_tags):\n",
    "        transition_p = transition_matrix.loc[tag,possible_tags[s]]\n",
    "        if transition_p == 0 : transition_p = np.finfo(float).tiny\n",
    "        val = viterbi_matrix[s1, t-1] + math.log(transition_p)\n",
    "        if val >= max_: max_ = val; argmax = s1\n",
    "    return max_, argmax\n",
    "\n",
    "def get_emission_p(emission_probabilities, word, tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags):\n",
    "    emission_p = 0\n",
    "    try:\n",
    "        count_word[word]\n",
    "    except KeyError: #unknown_word\n",
    "        emission_p = unknown_word_emission_p(smoothing_strategy, tag, possible_tags, oneshot_words_tag_distribution)         \n",
    "        return emission_p\n",
    "    try:\n",
    "        emission_p = emission_probabilities[(word,tag)]\n",
    "    except KeyError: #tag never emitted word\n",
    "        emission_p = 0\n",
    "    return emission_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0136673",
   "metadata": {},
   "source": [
    "FUNZIONI DI SMOOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ecd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_word_emission_p(smoothing_strategy,tag,possible_tags,oneshot_words_tag_distribution):\n",
    "    emission_p = 0\n",
    "    if smoothing_strategy.name == 'UNKNOWN_NAME':\n",
    "        if tag == 'NOUN': \n",
    "            emission_p = 1\n",
    "    if smoothing_strategy.name == 'UNKNOWN_NAME_VERB':\n",
    "        if tag == 'NOUN' or tag == 'VERB': \n",
    "            emission_p = 0.5\n",
    "    if smoothing_strategy.name == 'UNKNOWN_TAG': \n",
    "        emission_p = 1/len(possible_tags)\n",
    "    if smoothing_strategy.name == 'UNKNOWN_DEV': \n",
    "        emission_p = get_prob(tag, oneshot_words_tag_distribution)\n",
    "    return emission_p\n",
    "\n",
    "def get_prob(tag,oneshot_words_tag_distribution):\n",
    "    for tag_p,prob in oneshot_words_tag_distribution:\n",
    "        if tag == tag_p:\n",
    "            return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d432d6c",
   "metadata": {},
   "source": [
    "ALGORITMO DI VITERBI SUL GRECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fff1b1-5641-4a25-9fd2-37073e23de75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: VITERBI\n",
      "Lingua:  GREEK\n",
      "\n",
      "Tipologia di smoothing:  UNKNOWN_DEV\n",
      "PoS Tag corretti:  15982\n",
      "PoS Tag sbagliati:  4977\n",
      "Totale parole valutate:  20959\n",
      "Errori per PoS Tag:  {'ADV': 1633, 'PRON': 494, 'VERB': 380, 'NOUN': 1172, 'ADJ': 968, 'CCONJ': 131, 'DET': 123, 'SCONJ': 49, 'ADP': 18, 'PUNCT': 4, 'NUM': 1, 'INTJ': 3, 'X': 1}\n",
      "\n",
      "Accuratezza:  76.25 %\n",
      "Tempo di esecuzione:  30.62  sec\n"
     ]
    }
   ],
   "source": [
    "class Smoothing(enum.Enum):\n",
    "    UNKNOWN_NAME = 1\n",
    "    UNKNOWN_NAME_VERB = 2\n",
    "    UNKNOWN_TAG = 3\n",
    "    UNKNOWN_DEV = 4\n",
    "    \n",
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "start = ['START']\n",
    "\n",
    "#scelta dello smoothing\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME_VERB\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_TAG\n",
    "smoothing_strategy = Smoothing.UNKNOWN_DEV\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.GREEK\n",
    "language.name == 'GREEK'\n",
    "train = open(\"Dataset/grc_perseus-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "dev = open(\"Dataset/grc_perseus-ud-dev.conllu\",\"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/grc_perseus-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['START','ADJ','ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON','SCONJ', 'VERB', 'X', 'PUNCT','END']\n",
    "\n",
    "#learning\n",
    "transition_matrix = pd.DataFrame(compute_transition_matrix(possible_tags, train), columns = list(possible_tags), index=list(possible_tags))\n",
    "emission_probabilities, count_words, count_words_tag = compute_emission_probabilities(train)\n",
    "train.close()\n",
    "oneshot_words_tag_distribution = compute_oneshot_words_distributions(possible_tags, dev)\n",
    "dev.close()\n",
    "\n",
    "#rimuovo stato iniziale e finale perchè non servono più\n",
    "possible_tags.remove('START')\n",
    "possible_tags.remove('END')\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]            \n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    result_tags = viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_words, smoothing_strategy, oneshot_words_tag_distribution)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]: checked_words = checked_words + 1\n",
    "        else: error_list.append(pos_token_list[j])    \n",
    "end = time.time()\n",
    "\n",
    "#statistiche\n",
    "print(\"Algoritmo: VITERBI\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"\\nTipologia di smoothing: \",smoothing_strategy.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"\\nAccuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee0353",
   "metadata": {},
   "source": [
    "ALGORITMO DI VITERBI SUL LATINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57c4d98-d567-4040-9ce0-742d01c11602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: VITERBI\n",
      "Lingua:  LATIN\n",
      "\n",
      "Tipologia di smoothing:  UNKNOWN_DEV\n",
      "PoS Tag corretti:  23409\n",
      "PoS Tag sbagliati:  670\n",
      "Totale parole valutate:  24079\n",
      "Errori per PoS Tag:  {'VERB': 159, 'PROPN': 184, 'ADV': 49, 'NUM': 15, 'PRON': 22, 'DET': 25, 'ADJ': 82, 'NOUN': 66, 'AUX': 29, 'CCONJ': 21, 'PUNCT': 4, 'SCONJ': 12, 'ADP': 2}\n",
      "\n",
      "Accuratezza:  97.22 %\n",
      "Tempo di esecuzione:  40.79  sec\n"
     ]
    }
   ],
   "source": [
    "class Smoothing(enum.Enum):\n",
    "    UNKNOWN_NAME = 1\n",
    "    UNKNOWN_NAME_VERB = 2\n",
    "    UNKNOWN_TAG = 3\n",
    "    UNKNOWN_DEV = 4\n",
    "\n",
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "start = ['START']\n",
    "\n",
    "#scelta dello smoothing\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME_VERB\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_TAG\n",
    "smoothing_strategy = Smoothing.UNKNOWN_DEV\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.LATIN\n",
    "language.name == 'LATIN'\n",
    "train = open(\"Dataset/la_llct-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "dev = open(\"Dataset/la_llct-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/la_llct-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['START','ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON','PROPN','PUNCT', 'SCONJ', 'VERB','X','END']\n",
    "\n",
    "#learning\n",
    "transition_matrix = pd.DataFrame(compute_transition_matrix(possible_tags, train), columns = list(possible_tags), index=list(possible_tags))\n",
    "emission_probabilities, count_words, count_words_tag = compute_emission_probabilities(train)\n",
    "train.close()\n",
    "oneshot_words_tag_distribution = compute_oneshot_words_distributions(possible_tags, dev)\n",
    "dev.close()\n",
    "\n",
    "#rimuovo stato iniziale e finale perchè non servono più\n",
    "possible_tags.remove('START')\n",
    "possible_tags.remove('END')\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]            \n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    result_tags = viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_words, smoothing_strategy, oneshot_words_tag_distribution)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]: checked_words = checked_words + 1\n",
    "        else: error_list.append(pos_token_list[j])\n",
    "end = time.time()\n",
    "\n",
    "#statistics\n",
    "print(\"Algoritmo: VITERBI\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"\\nTipologia di smoothing: \",smoothing_strategy.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"\\nAccuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173b112",
   "metadata": {},
   "source": [
    "ALGORITMO DI BASELINE SUL GRECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24fa0281-9ac8-4f25-ae38-10360f3323b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: BASELINE\n",
      "Lingua:  GREEK\n",
      "\n",
      "PoS Tag corretti:  15411\n",
      "PoS Tag sbagliati:  5548\n",
      "Totale parole valutate:  20959\n",
      "Errori per PoS Tag:  {'VERB': 1978, 'ADV': 1823, 'PRON': 462, 'ADJ': 965, 'CCONJ': 133, 'DET': 88, 'SCONJ': 25, 'NOUN': 50, 'ADP': 16, 'NUM': 1, 'PUNCT': 3, 'INTJ': 3, 'X': 1}\n",
      "\n",
      "Accuratezza:  73.53 %\n",
      "Tempo di esecuzione:  0.32  sec\n"
     ]
    }
   ],
   "source": [
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.GREEK\n",
    "language.name == 'GREEK'\n",
    "train = open(\"Dataset/grc_perseus-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/grc_perseus-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['ADJ','ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON','SCONJ', 'VERB', 'X', 'PUNCT']\n",
    "\n",
    "count_words_tag = compute_emission_probabilities(train)[2]\n",
    "\n",
    "def baseline_algorithm(sentence_tokens,count_words_tag,possible_tags):\n",
    "    tags = []\n",
    "    for word in sentence_tokens:\n",
    "        tag_max = 'NOUN'\n",
    "        count_max_tag = 0\n",
    "        for tag in possible_tags:\n",
    "            if count_words_tag.get((word,tag),0) > count_max_tag:\n",
    "                count_max_tag = count_words_tag[word,tag]\n",
    "                tag_max = tag    \n",
    "        tags.append(tag_max)\n",
    "    return tags\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    result_tags = baseline_algorithm(sentence_tokens, count_words_tag, possible_tags)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]:\n",
    "            checked_words = checked_words + 1     \n",
    "        else:\n",
    "            error_list.append(pos_token_list[j])\n",
    "end = time.time()\n",
    "test.close()\n",
    "\n",
    "print(\"Algoritmo: BASELINE\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"\\nPoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"\\nAccuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24770cdf",
   "metadata": {},
   "source": [
    "ALGORITMO DI BASELINE SUL LATINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a61f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: BASELINE\n",
      "Lingua:  LATIN\n",
      "\n",
      "PoS Tag corretti:  22969\n",
      "PoS Tag sbagliati:  1110\n",
      "Totale parole valutate:  24079\n",
      "Errori per PoS Tag:  {'VERB': 248, 'PROPN': 471, 'ADV': 56, 'DET': 142, 'NUM': 34, 'ADJ': 83, 'NOUN': 15, 'CCONJ': 35, 'ADP': 6, 'SCONJ': 15, 'AUX': 3, 'PRON': 2}\n",
      "\n",
      "Accuratezza:  95.39 %\n",
      "Tempo di esecuzione:  0.39  sec\n"
     ]
    }
   ],
   "source": [
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.LATIN\n",
    "language.name == 'LATIN'\n",
    "train = open(\"Dataset/la_llct-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/la_llct-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON','PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "count_words_tag = compute_emission_probabilities(train)[2]\n",
    "\n",
    "def baseline_algorithm(sentence_tokens,count_words_tag,possible_tags):\n",
    "    tags = []\n",
    "    for word in sentence_tokens:\n",
    "        tag_max = 'NOUN'\n",
    "        count_max_tag = 0\n",
    "        for tag in possible_tags:\n",
    "            if count_words_tag.get((word,tag),0) > count_max_tag:\n",
    "                count_max_tag = count_words_tag[word,tag]\n",
    "                tag_max = tag    \n",
    "        tags.append(tag_max)\n",
    "    return tags\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    result_tags = baseline_algorithm(sentence_tokens, count_words_tag, possible_tags)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]:\n",
    "            checked_words = checked_words + 1     \n",
    "        else:\n",
    "            error_list.append(pos_token_list[j])\n",
    "end = time.time()\n",
    "test.close()\n",
    "\n",
    "print(\"Algoritmo: BASELINE\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"\\nPoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"\\nAccuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
