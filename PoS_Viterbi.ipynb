{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1bb3646-7790-4c58-9ab6-2c484b57a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from conllu import parse_incr\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6ed2f",
   "metadata": {},
   "source": [
    "1. FASE DI MODELLING: Viene fornito un modello formale del problema affrontato (già noto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c00595",
   "metadata": {},
   "source": [
    "2. FASE DI LEARNING: Si cerca di capire come, dato un corpus, sia possibile settare i parametri in grado di generare il modello in grado di apprendere da un corpus.\n",
    "\n",
    "   DEFINIZIONE DELLA MATRICE DI TRANSIZIONE E DELLA MATRICE DI EMISSIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17b47ffc-8558-4123-9d47-6322cb21e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(possible_tags, train):\n",
    "    transition_matrix = np.zeros((len(possible_tags), len(possible_tags)), dtype='float32')\n",
    "    \n",
    "    transition_counter_dict = dict()\n",
    "    counter_dict = dict()\n",
    "    count_initial_dict = dict()\n",
    "    count_final_dict = dict()\n",
    "\n",
    "    #FASE 1\n",
    "    for tag1 in possible_tags:\n",
    "        counter_dict[tag1] = 0\n",
    "        count_initial_dict[tag1] = 0\n",
    "        for tag2 in possible_tags:\n",
    "            transition_counter_dict[(tag1, tag2)] = 0\n",
    "\n",
    "    #FASE 2\n",
    "    sentence_n = 0\n",
    "    for sentence in parse_incr(train):\n",
    "        sentence_n += 1\n",
    "        for i in range(len(sentence)):\n",
    "            word_before = sentence[i-1]\n",
    "            word = sentence[i]\n",
    "            if i == 0:\n",
    "                if word[\"upos\"] in count_initial_dict.keys():\n",
    "                    count_initial_dict[word[\"upos\"]] = count_initial_dict[word[\"upos\"]] + 1\n",
    "            if (word_before[\"upos\"], word[\"upos\"]) in transition_counter_dict.keys() and i != 0:\n",
    "                transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] = transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] + 1\n",
    "            if word[\"upos\"] in counter_dict.keys():\n",
    "                counter_dict[word[\"upos\"]] = counter_dict[word[\"upos\"]] + 1\n",
    "            if i == len(sentence) - 1:\n",
    "                if (word[\"upos\"], 'END') in transition_counter_dict.keys():\n",
    "                    transition_counter_dict[(word[\"upos\"], 'END')] = transition_counter_dict[(word_before[\"upos\"], word[\"upos\"])] + 1\n",
    "    \n",
    "    #FASE 3: Calcolo delle probabilità di emissione delle parole iniziali, intermedie e finale per ottenere delle performance migliori\n",
    "    #probabilità di transizione iniziali\n",
    "    for i,t in enumerate(possible_tags):\n",
    "        transition_matrix[0][i] = count_initial_dict[t]/sentence_n\n",
    "    #probabilità di transizione intermedie\n",
    "    for i,t1 in enumerate(possible_tags):\n",
    "        for j,t2 in enumerate(possible_tags):\n",
    "            if i >= 1 and j >= 1 and i < (len(possible_tags) - 1):\n",
    "                transition_matrix[i][j] =  transition_counter_dict[(t1,t2)]/counter_dict[t1]\n",
    "    #probabilità di transizione finali\n",
    "    #for i,t in enumerate(possible_tags):\n",
    "    #    transition_matrix[i][len(possible_tags) - 1] = count_final_dict[t]/sentence_n\n",
    "    train.seek(0)\n",
    "    return transition_matrix\n",
    "\n",
    "#una_tantum -> serializzare\n",
    "def compute_emission_probabilities(train):\n",
    "    word_tag_set = []\n",
    "    tags_set = []\n",
    "    words_set = []\n",
    "    for sentence in parse_incr(train):\n",
    "        for token in sentence:\n",
    "            word_tag_set.append((token[\"form\"],token[\"upos\"]))\n",
    "            tags_set.append(token[\"upos\"])\n",
    "            words_set.append(token[\"form\"])\n",
    "            \n",
    "    count_word_tag = dict(Counter(word_tag_set))\n",
    "    count_tags = dict(Counter(tags_set))\n",
    "    count_word = dict(Counter(words_set))\n",
    "    \n",
    "    emission_dict = dict()\n",
    "    for key in count_word_tag:\n",
    "        emission_dict[(key[0],key[1])] = count_word_tag[key]/count_tags[key[1]]\n",
    "    return emission_dict,count_word,count_word_tag\n",
    "\n",
    "#una_tantum -> serializzare\n",
    "def compute_oneshot_words_distributions(possible_tags, dev):\n",
    "    word_tag_set = []\n",
    "    word_set = []\n",
    "    for sentence in parse_incr(dev):\n",
    "        for token in sentence:\n",
    "            word_tag_set.append((token[\"form\"],token[\"upos\"]))\n",
    "            word_set.append(token[\"form\"])\n",
    "    word_tag = dict(word_tag_set)\n",
    "    count_word = dict(Counter(word_set))\n",
    "    one_shot_words_tag = []\n",
    "    for word in [k for k,v in count_word.items() if float(v) == 1]:\n",
    "        one_shot_words_tag.append((word,word_tag[word]))\n",
    "    \n",
    "    tags = []\n",
    "    total_tags = 0\n",
    "    for word,tag in one_shot_words_tag:\n",
    "        tags.append(tag)\n",
    "        total_tags = total_tags + 1\n",
    "    distributions = []\n",
    "    for key,count in dict(Counter(tags)).items():\n",
    "        distributions.append((key,count/total_tags))\n",
    "    for tag in possible_tags:\n",
    "        if tag not in tags:\n",
    "            distributions.append((tag,0))\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe162f",
   "metadata": {},
   "source": [
    "3. FASE DI DECODING: Trovare l'algoritmo che permette di sfruttare al meglio i parametri appresi durante la fase di learning, per poter recuperare la soluzione ottimale dato un certo input.\n",
    "   \n",
    "   ALGORITMO DI VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a44df67a-0c8a-4ade-9a3d-7838695e18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_word, smoothing_strategy, oneshot_words_tag_distribution):\n",
    "    \n",
    "    viterbi_matrix = np.zeros((len(possible_tags), len(sentence_tokens))) #matrice di viterbi\n",
    "    backpointer = dict() #dizionario di dizionari\n",
    "    \n",
    "    #FASE 1: inizializzazione della prima colonna\n",
    "    for s,tag in enumerate(possible_tags):\n",
    "        transition_p = transition_matrix.loc['START',tag]\n",
    "        emission_p = get_emission_p(emission_probabilities, sentence_tokens[0], tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags)\n",
    "        \n",
    "        if transition_p == 0 : transition_p = np.finfo(float).tiny\n",
    "        if emission_p == 0 : emission_p = np.finfo(float).tiny\n",
    "        \n",
    "        viterbi_matrix[s,0] = math.log(transition_p) +  math.log(emission_p) \n",
    "        \n",
    "    #FASE 2: Inizializzazione delle colonne intermedie\n",
    "    #Si cicla prima sulle colonne e poi sulle righe \n",
    "    for t in range(1,len(sentence_tokens)):\n",
    "        backpointer_column = dict()\n",
    "        for s, tag in enumerate(possible_tags):\n",
    "            max_ , backpointer_column[s] = get_max_argmax_value(possible_tags, viterbi_matrix, transition_matrix, t, s)\n",
    "            emission_p = get_emission_p(emission_probabilities, sentence_tokens[t], tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags)\n",
    "            if emission_p == 0: emission_p = np.finfo(float).tiny\n",
    "            viterbi_matrix[s,t] = max_ + math.log(emission_p) \n",
    "        backpointer[t] = backpointer_column   \n",
    "    \n",
    "    #FASE 2: step finale (argmax)\n",
    "    max_ = -sys.maxsize\n",
    "    best_path_pointer = None\n",
    "    for s,tag in enumerate(possible_tags):\n",
    "        end_transition = transition_matrix.loc[tag,'END']\n",
    "        if end_transition == 0: end_transition = np.finfo(float).tiny\n",
    "        val = viterbi_matrix[s,len(sentence_tokens) - 1] + math.log(end_transition)\n",
    "        if val >= max_: max_ = val ; best_path_pointer = s\n",
    "    \n",
    "    #FASE 3: backtracking\n",
    "    #Recupero tramite backtracking della sequenza di PoS\n",
    "    states = []\n",
    "    states.append(best_path_pointer)\n",
    "    t = len(sentence_tokens) - 1\n",
    "    s = best_path_pointer\n",
    "    while t >= 1:\n",
    "        states.append(backpointer[t].get(s))\n",
    "        s = backpointer[t].get(s)\n",
    "        t = t -1\n",
    "    \n",
    "    #FASE 4: reverse PoS_Tag sequence\n",
    "    pos_tags_sequence = []\n",
    "    for state in list(reversed(states)): pos_tags_sequence.append(possible_tags[state])\n",
    "    return pos_tags_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648389",
   "metadata": {},
   "source": [
    "FUNZIONI DI SUPPORTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0457c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_argmax_value(possible_tags, viterbi_matrix, transition_matrix, t, s):\n",
    "    max_ = -sys.maxsize\n",
    "    argmax = None\n",
    "    for s1, tag in enumerate(possible_tags):\n",
    "        transition_p = transition_matrix.loc[tag,possible_tags[s]]\n",
    "        if transition_p == 0 : transition_p = np.finfo(float).tiny\n",
    "        val = viterbi_matrix[s1, t-1] + math.log(transition_p)\n",
    "        if val >= max_: max_ = val; argmax = s1\n",
    "    return max_, argmax\n",
    "\n",
    "def get_emission_p(emission_probabilities, word, tag, count_word, smoothing_strategy, oneshot_words_tag_distribution, possible_tags):\n",
    "    emission_p = 0\n",
    "    try:\n",
    "        count_word[word]\n",
    "    except KeyError: #unknown_word\n",
    "        emission_p = unknown_word_emission_p(smoothing_strategy, tag, possible_tags, oneshot_words_tag_distribution)         \n",
    "        return emission_p\n",
    "    try:\n",
    "        emission_p = emission_probabilities[(word,tag)]\n",
    "    except KeyError: #tag never emitted word\n",
    "        emission_p = 0\n",
    "    return emission_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0136673",
   "metadata": {},
   "source": [
    "FUNZIONI DI SMOOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69ecd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unknown_word_emission_p(smoothing_strategy,tag,possible_tags,oneshot_words_tag_distribution):\n",
    "    emission_p = 0\n",
    "    if smoothing_strategy.name == 'UNKNOWN_NAME':\n",
    "        if tag == 'NOUN': \n",
    "            emission_p = 1\n",
    "    if smoothing_strategy.name == 'UNKNOWN_NAME_VERB':\n",
    "        if tag == 'NOUN' or tag == 'VERB': \n",
    "            emission_p = 0.5\n",
    "    if smoothing_strategy.name == 'UNKNOWN_TAG': \n",
    "        emission_p = 1/len(possible_tags)\n",
    "    if smoothing_strategy.name == 'UNKNOWN_DEV': \n",
    "        emission_p = get_prob(tag, oneshot_words_tag_distribution)\n",
    "    return emission_p\n",
    "\n",
    "def get_prob(tag,oneshot_words_tag_distribution):\n",
    "    for tag_p,prob in oneshot_words_tag_distribution:\n",
    "        if tag == tag_p:\n",
    "            return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d432d6c",
   "metadata": {},
   "source": [
    "ALGORITMO DI VITERBI SUL GRECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79fff1b1-5641-4a25-9fd2-37073e23de75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: VITERBI\n",
      "Lingua:  GREEK\n",
      "Tipologia di smoothing:  UNKNOWN_DEV\n",
      "PoS Tag corretti:  15982\n",
      "PoS Tag sbagliati:  4977\n",
      "Totale parole valutate:  20959\n",
      "Errori per PoS Tag:  {'ADV': 1633, 'PRON': 494, 'VERB': 380, 'NOUN': 1172, 'ADJ': 968, 'CCONJ': 131, 'DET': 123, 'SCONJ': 49, 'ADP': 18, 'PUNCT': 4, 'NUM': 1, 'INTJ': 3, 'X': 1}\n",
      "Accuratezza:  76.25 %\n",
      "Tempo di esecuzione:  30.59  sec\n"
     ]
    }
   ],
   "source": [
    "class Smoothing(enum.Enum):\n",
    "    UNKNOWN_NAME = 1\n",
    "    UNKNOWN_NAME_VERB = 2\n",
    "    UNKNOWN_TAG = 3\n",
    "    UNKNOWN_DEV = 4\n",
    "    \n",
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "start = ['START']\n",
    "\n",
    "#scelta dello smoothing\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME_VERB\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_TAG\n",
    "smoothing_strategy = Smoothing.UNKNOWN_DEV\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.GREEK\n",
    "language.name == 'GREEK'\n",
    "train = open(\"Dataset/grc_perseus-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "dev = open(\"Dataset/grc_perseus-ud-dev.conllu\",\"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/grc_perseus-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['START','ADJ','ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON','SCONJ', 'VERB', 'X', 'PUNCT','END']\n",
    "\n",
    "#learning\n",
    "transition_matrix = pd.DataFrame(compute_transition_matrix(possible_tags, train), columns = list(possible_tags), index=list(possible_tags))\n",
    "emission_probabilities, count_words, count_words_tag = compute_emission_probabilities(train)\n",
    "train.close()\n",
    "oneshot_words_tag_distribution = compute_oneshot_words_distributions(possible_tags, dev)\n",
    "dev.close()\n",
    "\n",
    "#rimuovo stato iniziale e finale perchè non servono più\n",
    "possible_tags.remove('START')\n",
    "possible_tags.remove('END')\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]            \n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    result_tags = viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_words, smoothing_strategy, oneshot_words_tag_distribution)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]: checked_words = checked_words + 1\n",
    "        else: error_list.append(pos_token_list[j])    \n",
    "end = time.time()\n",
    "\n",
    "#statistiche\n",
    "print(\"Algoritmo: VITERBI\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"Tipologia di smoothing: \",smoothing_strategy.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"Accuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee0353",
   "metadata": {},
   "source": [
    "ALGORITMO DI VITERBI SUL LATINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c57c4d98-d567-4040-9ce0-742d01c11602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: VITERBI\n",
      "Lingua:  LATIN\n",
      "Tipologia di smoothing:  UNKNOWN_DEV\n",
      "PoS Tag corretti:  23409\n",
      "PoS Tag sbagliati:  670\n",
      "Totale parole valutate:  24079\n",
      "Errori per PoS Tag:  {'VERB': 159, 'PROPN': 184, 'ADV': 49, 'NUM': 15, 'PRON': 22, 'DET': 25, 'ADJ': 82, 'NOUN': 66, 'AUX': 29, 'CCONJ': 21, 'PUNCT': 4, 'SCONJ': 12, 'ADP': 2}\n",
      "Accuratezza:  97.22 %\n",
      "Tempo di esecuzione:  40.92  sec\n"
     ]
    }
   ],
   "source": [
    "class Smoothing(enum.Enum):\n",
    "    UNKNOWN_NAME = 1\n",
    "    UNKNOWN_NAME_VERB = 2\n",
    "    UNKNOWN_TAG = 3\n",
    "    UNKNOWN_DEV = 4\n",
    "    \n",
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "start = ['START']\n",
    "\n",
    "#scelta dello smoothing\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_NAME_VERB\n",
    "#smoothing_strategy = Smoothing.UNKNOWN_TAG\n",
    "smoothing_strategy = Smoothing.UNKNOWN_DEV\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.LATIN\n",
    "language.name == 'LATIN'\n",
    "train = open(\"Dataset/la_llct-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "dev = open(\"Dataset/la_llct-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/la_llct-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['START','ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON','PROPN','PUNCT', 'SCONJ', 'VERB','X','END']\n",
    "\n",
    "#learning\n",
    "transition_matrix = pd.DataFrame(compute_transition_matrix(possible_tags, train), columns = list(possible_tags), index=list(possible_tags))\n",
    "emission_probabilities, count_words, count_words_tag = compute_emission_probabilities(train)\n",
    "train.close()\n",
    "oneshot_words_tag_distribution = compute_oneshot_words_distributions(possible_tags, dev)\n",
    "dev.close()\n",
    "\n",
    "#rimuovo stato iniziale e finale perchè non servono più\n",
    "possible_tags.remove('START')\n",
    "possible_tags.remove('END')\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]            \n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    result_tags = viterbi_algorithm(sentence_tokens, possible_tags, transition_matrix, emission_probabilities, count_words, smoothing_strategy, oneshot_words_tag_distribution)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]: checked_words = checked_words + 1\n",
    "        else: error_list.append(pos_token_list[j])    \n",
    "end = time.time()\n",
    "\n",
    "#statistics\n",
    "print(\"Algoritmo: VITERBI\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"Tipologia di smoothing: \",smoothing_strategy.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"Accuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173b112",
   "metadata": {},
   "source": [
    "ALGORITMO DI BASELINE SUL GRECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24fa0281-9ac8-4f25-ae38-10360f3323b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: BASELINE\n",
      "Lingua:  GREEK\n",
      "PoS Tag corretti:  15411\n",
      "PoS Tag sbagliati:  5548\n",
      "Totale parole valutate:  20959\n",
      "Errori per PoS Tag:  {'VERB': 1978, 'ADV': 1823, 'PRON': 462, 'ADJ': 965, 'CCONJ': 133, 'DET': 88, 'SCONJ': 25, 'NOUN': 50, 'ADP': 16, 'NUM': 1, 'PUNCT': 3, 'INTJ': 3, 'X': 1}\n",
      "Accuratezza:  73.53 %\n",
      "Tempo di esecuzione:  0.33  sec\n"
     ]
    }
   ],
   "source": [
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.GREEK\n",
    "language.name == 'GREEK'\n",
    "train = open(\"Dataset/grc_perseus-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/grc_perseus-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['ADJ','ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON','SCONJ', 'VERB', 'X', 'PUNCT']\n",
    "\n",
    "count_words_tag = compute_emission_probabilities(train)[2]\n",
    "\n",
    "def baseline_algorithm(sentence_tokens,count_words_tag,possible_tags):\n",
    "    tags = []\n",
    "    for word in sentence_tokens:\n",
    "        tag_max = 'NOUN'\n",
    "        count_max_tag = 0\n",
    "        for tag in possible_tags:\n",
    "            if count_words_tag.get((word,tag),0) > count_max_tag:\n",
    "                count_max_tag = count_words_tag[word,tag]\n",
    "                tag_max = tag    \n",
    "        tags.append(tag_max)\n",
    "    return tags\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    result_tags = baseline_algorithm(sentence_tokens, count_words_tag, possible_tags)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]:\n",
    "            checked_words = checked_words + 1     \n",
    "        else:\n",
    "            error_list.append(pos_token_list[j])\n",
    "end = time.time()\n",
    "test.close()\n",
    "\n",
    "print(\"Algoritmo: BASELINE\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"Accuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24770cdf",
   "metadata": {},
   "source": [
    "ALGORITMO DI BASELINE SUL LATINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3a61f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo: BASELINE\n",
      "Lingua:  LATIN\n",
      "PoS Tag corretti:  22969\n",
      "PoS Tag sbagliati:  1110\n",
      "Totale parole valutate:  24079\n",
      "Errori per PoS Tag:  {'VERB': 248, 'PROPN': 471, 'ADV': 56, 'DET': 142, 'NUM': 34, 'ADJ': 83, 'NOUN': 15, 'CCONJ': 35, 'ADP': 6, 'SCONJ': 15, 'AUX': 3, 'PRON': 2}\n",
      "Accuratezza:  95.39 %\n",
      "Tempo di esecuzione:  0.37  sec\n"
     ]
    }
   ],
   "source": [
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "\n",
    "#scelta della lingua\n",
    "language = Language.LATIN\n",
    "language.name == 'LATIN'\n",
    "train = open(\"Dataset/la_llct-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"Dataset/la_llct-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "possible_tags = ['ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON','PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "count_words_tag = compute_emission_probabilities(train)[2]\n",
    "\n",
    "def baseline_algorithm(sentence_tokens,count_words_tag,possible_tags):\n",
    "    tags = []\n",
    "    for word in sentence_tokens:\n",
    "        tag_max = 'NOUN'\n",
    "        count_max_tag = 0\n",
    "        for tag in possible_tags:\n",
    "            if count_words_tag.get((word,tag),0) > count_max_tag:\n",
    "                count_max_tag = count_words_tag[word,tag]\n",
    "                tag_max = tag    \n",
    "        tags.append(tag_max)\n",
    "    return tags\n",
    "\n",
    "#testing di tutte le sentence del test set\n",
    "#Calcolo l'accuracy e i tempi di esecuzione dell'algoritmo di PoS tagging.\n",
    "checked_words = 0\n",
    "tested_words_n = 0\n",
    "error_list = []\n",
    "start = time.time()\n",
    "for sentence in parse_incr(test):\n",
    "    pos_token_list = [token[\"upos\"] for token in sentence]\n",
    "    sentence_tokens = [token[\"form\"] for token in sentence]\n",
    "    tested_words_n = tested_words_n + len(pos_token_list)\n",
    "    result_tags = baseline_algorithm(sentence_tokens, count_words_tag, possible_tags)\n",
    "    for j in range(len(pos_token_list)):\n",
    "        if pos_token_list[j] == result_tags[j]:\n",
    "            checked_words = checked_words + 1     \n",
    "        else:\n",
    "            error_list.append(pos_token_list[j])\n",
    "end = time.time()\n",
    "test.close()\n",
    "\n",
    "print(\"Algoritmo: BASELINE\")\n",
    "print(\"Lingua: \", language.name)\n",
    "print(\"PoS Tag corretti: \", checked_words)\n",
    "print(\"PoS Tag sbagliati: \", tested_words_n - checked_words)\n",
    "print(\"Totale parole valutate: \",tested_words_n)\n",
    "print(\"Errori per PoS Tag: \", dict(Counter(error_list)))\n",
    "print(\"Accuratezza: \", format((checked_words/tested_words_n)*100,'.2f'),\"%\")\n",
    "print(\"Tempo di esecuzione: \", format(end - start,'.2f'),\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc184c50-834b-4473-b445-094ae42d0a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>END</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>START</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046508</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>0.070243</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.191384</td>\n",
       "      <td>0.034984</td>\n",
       "      <td>0.139388</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.025792</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.378241</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089186</td>\n",
       "      <td>0.022273</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.047578</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.283196</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.213819</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.034499</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090429</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244686</td>\n",
       "      <td>0.353729</td>\n",
       "      <td>0.006355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105331</td>\n",
       "      <td>0.147677</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.040659</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011566</td>\n",
       "      <td>0.214399</td>\n",
       "      <td>0.048576</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.080382</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.029059</td>\n",
       "      <td>0.061009</td>\n",
       "      <td>0.129536</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.010843</td>\n",
       "      <td>0.301720</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089726</td>\n",
       "      <td>0.109915</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.061014</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.154329</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>0.275011</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.229699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054470</td>\n",
       "      <td>0.157828</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.089943</td>\n",
       "      <td>0.263798</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.054560</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>0.175925</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.056543</td>\n",
       "      <td>0.076478</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.055128</td>\n",
       "      <td>0.447487</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.033225</td>\n",
       "      <td>0.076724</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.083308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.092977</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.076263</td>\n",
       "      <td>0.151512</td>\n",
       "      <td>0.082509</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.077879</td>\n",
       "      <td>0.180141</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.151778</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.189908</td>\n",
       "      <td>0.034183</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.421595</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.059685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.223265</td>\n",
       "      <td>0.043152</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.071295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.082030</td>\n",
       "      <td>0.049471</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.039115</td>\n",
       "      <td>0.100954</td>\n",
       "      <td>0.139920</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.079049</td>\n",
       "      <td>0.253241</td>\n",
       "      <td>0.020861</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.191030</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.049331</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.373515</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.026260</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.371764</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.055771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021844</td>\n",
       "      <td>0.072242</td>\n",
       "      <td>0.029219</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.041871</td>\n",
       "      <td>0.029498</td>\n",
       "      <td>0.206801</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.173389</td>\n",
       "      <td>0.019223</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>0.089753</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.244303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.260805</td>\n",
       "      <td>0.219374</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.070045</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.200894</td>\n",
       "      <td>0.014307</td>\n",
       "      <td>0.047988</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.110581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012123</td>\n",
       "      <td>0.165639</td>\n",
       "      <td>0.018164</td>\n",
       "      <td>0.037119</td>\n",
       "      <td>0.115189</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.088735</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.053699</td>\n",
       "      <td>0.037494</td>\n",
       "      <td>0.291160</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.138477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>END</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       START       ADJ       ADP       ADV       AUX     CCONJ       DET  \\\n",
       "START    0.0  0.046508  0.022911  0.070243  0.000412  0.191384  0.034984   \n",
       "ADJ      0.0  0.089186  0.022273  0.005023  0.034594  0.047578  0.006729   \n",
       "ADP      0.0  0.090429  0.002025  0.008323  0.000000  0.000000  0.244686   \n",
       "ADV      0.0  0.011566  0.214399  0.048576  0.017782  0.013879  0.043516   \n",
       "AUX      0.0  0.089726  0.109915  0.013459  0.000000  0.026021  0.018394   \n",
       "CCONJ    0.0  0.054470  0.157828  0.050239  0.004051  0.007563  0.089943   \n",
       "DET      0.0  0.061589  0.056543  0.076478  0.001415  0.024611  0.055128   \n",
       "NOUN     0.0  0.101177  0.092977  0.023564  0.007742  0.076263  0.151512   \n",
       "NUM      0.0  0.015735  0.189908  0.034183  0.003256  0.051546  0.010309   \n",
       "PART     0.0  0.000000  0.015009  0.011257  0.223265  0.043152  0.001876   \n",
       "PRON     0.0  0.005215  0.082030  0.049471  0.020414  0.039115  0.100954   \n",
       "PROPN    0.0  0.032700  0.044267  0.019945  0.002438  0.049331  0.012192   \n",
       "PUNCT    0.0  0.021844  0.072242  0.029219  0.000454  0.041871  0.029498   \n",
       "SCONJ    0.0  0.012817  0.260805  0.219374  0.002086  0.004471  0.070045   \n",
       "VERB     0.0  0.012123  0.165639  0.018164  0.037119  0.115189  0.016997   \n",
       "X        0.0  0.012658  0.050633  0.000000  0.000000  0.000000  0.000000   \n",
       "END      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           NOUN       NUM      PART      PRON     PROPN     PUNCT     SCONJ  \\\n",
       "START  0.139388  0.001372  0.000549  0.025792  0.009878  0.378241  0.003841   \n",
       "ADJ    0.283196  0.000758  0.000284  0.008435  0.213819  0.252867  0.000569   \n",
       "ADP    0.353729  0.006355  0.000000  0.105331  0.147677  0.000562  0.000169   \n",
       "ADV    0.080382  0.005060  0.029059  0.061009  0.129536  0.032529  0.010843   \n",
       "AUX    0.061014  0.001795  0.000449  0.154329  0.014805  0.275011  0.005384   \n",
       "CCONJ  0.263798  0.029621  0.010354  0.037364  0.054560  0.004952  0.059242   \n",
       "DET    0.447487  0.007629  0.000554  0.033225  0.076724  0.060604  0.014705   \n",
       "NOUN   0.082509  0.018716  0.000892  0.027326  0.077879  0.180141  0.007091   \n",
       "NUM    0.421595  0.022789  0.000000  0.003798  0.000543  0.185567  0.001085   \n",
       "PART   0.071295  0.000000  0.000000  0.009381  0.001876  0.000000  0.000000   \n",
       "PRON   0.139920  0.009835  0.001714  0.079049  0.253241  0.020861  0.003129   \n",
       "PROPN  0.373515  0.000813  0.000313  0.026260  0.008128  0.371764  0.002563   \n",
       "PUNCT  0.206801  0.001957  0.000454  0.173389  0.019223  0.002551  0.055956   \n",
       "SCONJ  0.035469  0.002385  0.002385  0.200894  0.014307  0.047988  0.016393   \n",
       "VERB   0.088735  0.008040  0.004749  0.053699  0.037494  0.291160  0.012415   \n",
       "X      0.240506  0.000000  0.000000  0.012658  0.000000  0.493671  0.012658   \n",
       "END    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           VERB         X       END  \n",
       "START  0.074359  0.000137  0.000000  \n",
       "ADJ    0.034499  0.000190  0.000000  \n",
       "ADP    0.040659  0.000056  0.000000  \n",
       "ADV    0.301720  0.000145  0.000000  \n",
       "AUX    0.229699  0.000000  0.000000  \n",
       "CCONJ  0.175925  0.000090  0.000000  \n",
       "DET    0.083308  0.000000  0.000000  \n",
       "NOUN   0.151778  0.000434  0.000000  \n",
       "NUM    0.059685  0.000000  0.000000  \n",
       "PART   0.622889  0.000000  0.000000  \n",
       "PRON   0.191030  0.004023  0.000000  \n",
       "PROPN  0.055771  0.000000  0.000000  \n",
       "PUNCT  0.089753  0.000035  0.244303  \n",
       "SCONJ  0.110581  0.000000  0.000000  \n",
       "VERB   0.138477  0.000000  0.000000  \n",
       "X      0.177215  0.000000  0.000000  \n",
       "END    0.000000  0.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Smoothing(enum.Enum):\n",
    "    UNKNOWN_NAME = 1\n",
    "    UNKNOWN_NAME_VERB = 2\n",
    "    UNKNOWN_TAG = 3\n",
    "    UNKNOWN_DEV = 4\n",
    "    \n",
    "class Language(enum.Enum):\n",
    "    GREEK = 1\n",
    "    LATIN = 2\n",
    "    \n",
    "start = ['START']\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "smoothing_strategy = Smoothing.UNKNOWN_DEV\n",
    "language = Language.LATIN\n",
    "#language = Language.GREEK\n",
    "\n",
    "if language.name == 'GREEK':\n",
    "    train = open(\"Dataset/grc_perseus-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    dev = open(\"Dataset/grc_perseus-ud-dev.conllu\",\"r\", encoding=\"utf-8\")\n",
    "    test = open(\"Dataset/grc_perseus-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    possible_tags = ['START','ADJ','ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON','SCONJ', 'VERB', 'X', 'PUNCT','END']\n",
    "elif language.name == 'LATIN':\n",
    "    train = open(\"Dataset/la_llct-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    dev = open(\"Dataset/la_llct-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    test = open(\"Dataset/la_llct-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    possible_tags = ['START','ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON','PROPN','PUNCT', 'SCONJ', 'VERB','X','END']\n",
    "\n",
    "transition_matrix = pd.DataFrame(compute_transition_matrix(possible_tags, train), columns = list(possible_tags), index=list(possible_tags))\n",
    "display(transition_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
